
# 1 什么是决策树

决策树是随机森林的基础，它既可以用于回归问题中，也可以用来解决分类问题。

为了方便大家理解什么是决策树，先从分类问题开始，举个简单的栗子，如下图所示。

![](http://ww1.sinaimg.cn/large/005N2p5vly1ftxpcpg425j315u16egtx.jpg)

我们的目的是将图片最上方的九个物品分类为黑色和白色。

但这九个物品又可以按照形状分类为：圆、三角、和矩形。再详细一点，又可以分类为横向椭圆、竖直椭圆，正方形、长方形。我们将这些分类称为这些物体的特征。

**在决策树的分类问题中，我们要通过区分不同的特征将这些物品分类为我们需要的黑色和白色。**

比如，首先根据这个物品是不是椭圆，分为两组。左边是椭圆，右边的物品不是椭圆，即包含了三角、矩形。这样就完成了第一次特征选择，以及第一次划分。但是这样划分出来的两个叶子分类包括的物品还是无法区分是黑色还是白色。于是我们继续进行划分。

对左边，我们又将椭圆分为横向椭圆和竖直椭圆。这样得到的两个叶子，包含的物品都是要么全是黑色，要么全是白色。这样左边就完成了分类。我们再看右边。

首先对右边进行形状的划分，分为三角和矩形。三角区全部都是黑色，不用再继续划分。

矩形区还可以再细分为正方形和长方形。这样也可以区分出白色物品和黑色物品。即完成了划分。这样，树中的所有叶子节点都是全部是黑色或者全部是白色物品。

按照这种方法，我们就成功的构建了一棵简单的分类决策树。


不过，在以上过程中，我们忽略了一个重要的问题，那就是在每一次划分中如何选取用哪个特征进行划分，这个问题被称为特征选择。特征选择的目的是：使划分出来的两个区域，所含元素的纯度尽量高。也就是在一个区域中，如果这个区域的所有节点，都属于同一类（黑色和白色），那么就说该区域纯度很高。

在决策树的生成中，第一个要解决的问题就是特征选择。

## 1.1 决策树生成之特征选择

特征选择有三种方式，分别是按照信息增益进行选择，按照信息增益比进行选择，以及按照基尼指数进行选择。三种特征选择方法分别对应于三种生成决策树的方法，分别为ID3算法、C4.5算法和CART中的生成。

不论是哪一种特征选择方法，其目的都是**令叶子节点的纯度尽可能的高，也就是使不确定性最低。**

### 1.1.1 使用信息增益进行特征选择

信息增益表示得知特征X的信息 而使得 类Y的信息的 不确定性减少的程度。

在使用信息增益进行特征选择时，要选择使信息增益最大的特征进行划分。


### 1.1.2 使用信息增益比进行特征选择

信息增益比：信息增益值的大小是相对于训练数据集的影响，并没有绝对意义。信息增益受熵的大小影响，与其成正比。

![](http://ww1.sinaimg.cn/large/005N2p5vly1ftxr8dh9z9j31j807c43i.jpg)

信息增益比消除了熵的影响。

![](http://ww1.sinaimg.cn/large/005N2p5vly1ftxrb5ijdlj31jk09mq7p.jpg)

在使用信息增益比进行特征选择时，也要选择使信息增益比最大的特征进行划分。

### 1.1.3 使用基尼系数进行特征选择

首先看一下基尼指数的定义：

![](http://ww1.sinaimg.cn/large/005N2p5vly1ftxriz6mhnj31pw09ugrg.jpg)

从定义中可以看出，当p趋近于0或1时，基尼系数都趋近于0。也就是说基尼系数越小，纯度越高。

因此，在使用基尼系数进行特征选择时，需要选择使基尼系数最小的特征。

## 1.2 决策树生成的步骤

决策树的生成有三种方式。这三种方式的区别在于进行特征选择时使用的方法不同。

![](http://ww1.sinaimg.cn/large/005N2p5vly1ftxrpmkqa6j30uq072jsk.jpg)

下面以ID3算法为例，说明一下决策树的生成过程：

![](http://ww1.sinaimg.cn/large/005N2p5vly1ftxrqqkc7fj31iw0dyqfv.jpg)

## 决策树的剪枝

至此为止，我们已经生成了一棵决策树。但是还没结束，现在的决策树还有一个严重的问题——过拟合。

决策树生成算法往往对训练数据的分类很准确，但这样可能会导致过拟合，即对未知的测试数据的预测没有那么准确。因此要将生成的决策树简化，降低其复杂度。此过程称为剪枝。

![](http://ww1.sinaimg.cn/large/005N2p5vly1ftxrx0cfauj317s15wain.jpg)

剪枝的过程———从已生成的树上裁掉一些子树或叶节点，并将其根节点或父节点作为新的叶节点，从而简化分类树模型。

在决策树的剪枝过程中，使用损失函数来决定裁掉哪些子树或叶子节点。损失函数定义如下：

![](http://ww1.sinaimg.cn/large/005N2p5vly1ftxrz4vgtuj30zw0ccjtc.jpg)

其中，α≥0

当确定α时
- 子树越大，拟合越好，但复杂度越高。
- 子树越小，复杂度越小，但拟合不好。

损失函数表示了对二者的平衡。

较大的α促使选择较简单的模型（树）

较小的α促使选择较复杂的模型（树）


总结一下决策树的生成与剪枝：生成时只考虑拟合度，是局部模型。但剪枝时既要考虑拟合又要考虑模型复杂度（通过优化损失函数），因此是个整体模型。

# bagging如何解决决策树的过拟合问题



# 随机森林解决bagging的重复特征选择问题




























































































