# 这篇文档的问题

- [ ] 不够系统：缺少对HDFS，MapReduce等的描述，对比不够详细
- [ ] 对技术的描述还可以再抽象一点
- [ ] 对spark的实现原理描述的不够全面——可靠性，实时，机器学习
- [ ] 开头可以添加一个段落单独说明大数据的发展史






## 大数据的实现——分布式

分布式：当进行数据的计算时，将需要处理的数据进行分区，使用**多台电脑并行处理**，这样能极大地缩减数据处理所需要花费的时间。分布式原理也可以应用于数据的存储，原理也是和计算一样，一台电脑的性能不足，所以将数据分片，存储在多台电脑上。

**Hadoop**的出现，使得数据处理与存储能够在多台性能较差的机器上实现。实现了分布式，以及提供了多个满足用户需求的组件。

### 分布式的原理
在一个集群中，有很多个节点（每一台电脑称为一个节点）。这些节点之中有一个Master节点，还有很多个Slave节点。\
Master节点用于资源的分配与管理。\
Slave节点用于数据的计算与存储。

Hadoop中用于计算的组件有两个：

1. MapReduce
2. Spark 

## Spark

简单的定义：Spark是一个高速的、用于分布式的计算系统。

### Spark的原理

- Spark将数据进行并行化处理。
    - 所谓并行化，就是将




----
### 数据来源：
1. 实时数据
2. 离线数据


### 大数据主要讨论：
1. 计算
    - Spark（离线+实时：Spark Streaming）
    - Storm（实时）
    - MapReduce（离线）

2. 存储
    - HDFS
    - HBase


### Spark 出现的原因 

MapReduce可以进行离线大数据的处理。
- 处理方式单一，只能使用一种模型
- 因为处理方式单一，所以需要job之间的复杂的依赖关系来实现更复杂的逻辑。
- job之间的复杂的依赖关系需要开发者管理。
- 速度慢（在硬盘上进行处理），中间结果存放在HDFS上。
- 只适用于批数据的处理。对于交互式数据处理、实时数据处理的支持不够
- 对于迭代式数据处理性能比较差

因此出现了Spark。Spark可以将MapReduce的处理速度提高10-100倍。
- 基于内存计算，中间结果可以保存在内存中，方便下次计算，减少了磁盘I/O，更适合迭代计算。
- 对比MapReduce单一的处理方式，Spark可以自定义处理的函数，更灵活。
- Spark提供了一个统一的处理平台——批处理（Spark Core）、交互式（Spark SQL）、流式（Spark Streaming）、机器学习（MLlib）、图计算（GraphX）


### 实时处理的技术对比

1. Storm
    - 实时度更高


2. Spark Streaming
    - 处理的数据量更大


>Storm与Spark Streaming都是基于内存计算。

>但是Storm对实时的处理机制是“事件触发”，有事件就可以触发。Spark Streaming对实时的处理机制是处理一定时间间隔内出现/更新的数据。

>Storm可以达到毫秒级的实时计算延迟度，而Spark Streaming只能达到秒级的计算延迟度。

>虽然Storm和Spark Streaming都是基于内存计算，但是Storm不支持对计算结果的保存。而Spark Streaming使用的计算原理是Spark core，也就是说能够实现数据的持久化。因此Spark Streaming的吞吐量更大。

#### 为什么Storm实时度更高？（Storm：毫秒，Spark Streaming：秒）

- Storm对实时的处理机制是“事件触发”，有事件就立即进行处理。
- Spark Streaming对实时的处理机制是：处理一定时间间隔内 出现/更新 的数据。

#### 为什么 Spark Streaming 吞吐量大

Spark Streaming 其实是对 Spark Core 的进一步加工：
- 对于数据的接收：接收一定时间间隔内的数据。
- 对于数据的处理：可以一次性处理大量数据，处理数据的原理采用Spark Core的处理原理，因此延续了 Spark Core 的特性。
- Spark Core 的一个特性是持久化，因此保证了 Spark Streaming 可以有比 Storm 更大的吞吐量。
- Storm 并没有存储系统，因此不能将数据持久化。能够处理的数据量要根据内存的容量。


## TODO：spark 调研——机器学习








































