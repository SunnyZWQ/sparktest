# 1 软件概述

此课题最终结果是对书籍评论信息作出处理，为每个用户推荐10本最可能喜欢的书籍。在web端为用户展示全部图书的书籍列表。书籍列表信息来自于从“豆瓣读书”爬到的数据。为用户做的推荐，是根据从“豆瓣读书”中爬到的评论信息，为了使推荐更准确，还使用了来自UCSD的公开数据集。数据处理端的工作主要分为两大类，一是对采集到的数据进行清洗、整合，二是根据清洗好的数据为用户做推荐，实现基于用户的协同过滤。

## 1.1 本系统的研究背景

Amazon.com 在 1998 年推出了基于项目的协同过滤算法（item-based collaborative filtering），使推荐系统能够基于上百万的商品目录为数百万用户提供推荐服务。2003 年，Greg Linden、Brent Smith 和 Jeremy York 将该算法发表在“IEEE Internet Computing”杂志上，论文名称为“Amazon.com Recommendations: Item-to-Item Collaborative Filtering”。随后，该算法在网络上被广泛地使用在不同的产品中，包括 Youtube、Netflix 和很多其他产品。该算法的成功在于：简单、可扩展、能提供让人惊奇且有用的推荐、能根据用户的新信息快速地更新推荐结果以及能够以易于理解的方式解释推荐内容。

在过去的十多年中，该算法面临了很多挑战，也得到了很多改进。这里，该算法的其中两名原作者 Brent Smith 和 Greg Linden 将描述其中的一些更新、改进和修改，并对协同过滤、推荐系统和个性化的未来进行展望。

## 1.2 市面上的其他类似软件

亚马逊、淘宝、京东、当当等各大电商网站都有自己的推荐系统，来增加平台的销量。

![](http://ww1.sinaimg.cn/large/005N2p5vly1frg05i08hmj327q1aunpe.jpg)

![](http://ww1.sinaimg.cn/large/005N2p5vly1frg06yt6ozj327o1bsb29.jpg)
 
![](http://ww1.sinaimg.cn/large/005N2p5vly1frg09806ywj327y1bwkjl.jpg)

## 1.3 课题主要工作

从豆瓣中爬取

# 2 技术选择

## 2.1 技术选择综述


爬虫：

- 操作系统：CentOS 7.3
- 语言：python
- 框架：Scrapy

web：

- 操作系统：CentOS 7.3
- 语言：python
- 框架：flask
- 前端：HTML，CSS
- 数据库：MySQL

批处理：
- Hadoop
- Spark
- Kappa架构的思想

推荐算法：
- 基于用户喜好的协同过滤

## 2.1 Scrapy爬虫

Scrapy框架的原理是简化了从网站获取元素内容的代码量。将本来复杂繁琐的Beaufiful Soup代码简化到只要用固定的格式就能抽取要获取的网站的元素，及其内容。

用来定位元素位置的选择器有两种方案，一种是xpath选择器，一种是CSS选择器。

两种选择器各有优势，xpath的优势是方便获取，缺点是对于网站布局在动态改变的情况，xpath的适应情况会差一点。CSS选择器的缺点是定位相对xpath复杂一些，但是也很容易获取，所以也不算是很困难。优点就是可以按照内容来爬取数据，而不是按照位置，这样大大增加了爬虫的灵活性，更能适应动态改变布局的网站。

综合考虑，选择用CSS选择器来爬取豆瓣读书的信息。

## 2.2 web端

前端采用HTML、CSS写网页，后端使用Python编写。Python编写的后台使用到的框架是flask。Python有很多搭建网站的框架，流行的如Django等，个人觉得flask更轻量一些，Django虽然功能全但是更复杂，仅仅搭建一个能连接数据库的后台不需要过多的功能。因此，服务器端选择使用flask框架。


## 2.3 批处理


Hadoop解决了大数据的处理问题主要是通过两方面——存储和计算。
Hadoop解决大数据计算问题的组件——MapReduce。MapReduce是最早期用于批处理的组件。它为开发者提供了一个编程模型。开发者只需要使用这个编程模型实现数据的计算，而不需要考虑分布式如何实现、如何容错等问题。

MapReduce解决了大数据的计算问题，但它并不是完美的，MapReduce的计算速度太慢了，如果对数据的处理时间有要求。比如说实时计算，那么MapReduce就不能满足需求。

MapReduce之所以计算速度特别慢，是因为它每次在计算的时候，首先从硬盘中取数据到内存，计算出中间结果之后，再将中间结果重新写入硬盘中。如果需要对中间结果进行下一步计算，又需要将中间结果重新从硬盘中读出并写入内存。
也就是说，MapReduce在执行过程中浪费了大量的I/O时间。

为了解决MapReduce进行大数据批处理时运算速度过慢的问题，Spark对其进行了很多改进。

Spark 在进行 批处理时主要用到的是 Spark core。
	

首先，是运算速度方面。刚才已经提到MapReduce在执行过程中浪费了大量的I/O时间，因此，Spark的改进就是中间结果不再放到硬盘中，而是继续存放在内存中。这样，下次再需要中间结果的时候就不需要消耗I/O时间。

Spark 做的第二个改进就是它提供的并不是单一的编程模型，而是可以使用自定义函数。这样就大大增加了Spark的易用性。
	
MapReduce提供的编程模型的逻辑非常简单，因此要实现非常复杂的逻辑，就要把这个简单的功能一层一层叠加。既然要叠加逻辑，就要考虑每一层之间的连接，如何设计每一层逻辑，来使这个编程模型更高效。
	
因此，MapReduce 的易用性非常差——仅仅使用MapReduce进行表的关联就需要非常复杂的逻辑。
	
接下来是Spark的第三个改进，就是Spark框架为批处理、交互式、流式、机器学习、图计算提供一个统一的数据处理平台，这相对于使用Hadoop有很大优势。Hadoop在使用时，如果想使用其余的组件，需要单独对这个组件配置环境，并且要考虑各个组件之间如何连接，如何高效的连接。	


## 2.4 推荐算法

### 2.4.1 机器学习简介

协同过滤算法是机器学习众多算法之一。那么首先说一下机器学习的流程。

![](http://ww1.sinaimg.cn/large/005N2p5vgy1fpy4h98u3kj30in0a4taf.jpg)

人工智能 指的就是 像人一样的思考、行为。而且是理性的思考，理性的行为。人在生活工作中会不断学习，但是机器不会。机器不能进行自身的调整以适应新的变化。因此，机器学习解决的问题就是——让计算机系统和人工智能系统具备学习环境，根据现行的工作环境进行调整以适应生产和科技的。
	
机器学习是实现人工智能的一个途径，即以机器学习为手段解决人工智能中的问题。学习就是系统经过不断重复的工作逐步改进和增强自身能力的过程，当系统在下一次遇到相同或相似的问题时，会比现在处理的更好或者更高效。
	
机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。因为学习算法中涉及了大量的统计学理论，机器学习与推断统计学联系尤为密切，也被称为统计学习理论。

机器学习的过程是：

1. 先对一些训练数据集使用机器学习的算法进行处理，找出数据的规律，找到的规律就是模型。
2. 再用模型对新的数据进行处理，就能得到预测数据。

### 2.4.2 基于用户的协同过滤的算法的原理

基于用户的协同过滤的算法的原理是通过计算用户与用户之间的相似度，来为用户推荐可能喜欢的书籍。这个算法的大致流程是：首先根据过往的用户行为，比如点击、搜索、收藏、付款，为每一个行为设计权重，然后将用户对每一件商品的喜好度评分根据各个行为的权重计算出来，形成一个（用户-物品）的评分矩阵。再用评分矩阵，计算用户与用户的相似度，利用的原理是向量与向量之间的余弦夹角。
![](http://ww1.sinaimg.cn/large/005N2p5vgy1fremljhvaoj311s0gqwie.jpg)




## 2.1 数据来源

爬取数据的网站是“豆瓣读书”，网址为https://book.douban.com/tag/，以及用于推荐的来自UCSD的亚马逊公开数据集中的书籍评论信息。web端展示书籍列表信息使用在“豆瓣读书”中爬到的数据，推荐数据使用“豆瓣读书”中爬到的评论信息，为了使推荐更准确，还使用了来自UCSD的公开数据集。

# 3 爬虫技术


## 3.1 爬虫设计思想、方法


爬取豆瓣读书的思路是这样的：

1、标签链接页面(testLink.py)。爬数据的初始网址中包含豆瓣中所有的图书标签和对应的链接，从标签点击进去能看到这个标签下所有的图书的可翻页的列表。

2、图书列表页面（可翻页）(testBookList.py)。在这个列表中，可爬取到书籍详情页链接、书名、作者、译者、出版社、出版年月日、图书价格等基本信息。

3、书籍详情页面(testEntry.py)。从书籍详情页面的链接进去，找到所有评论的链接。

4、数据评论页面(testComment.py)。在评论页面能够看到本书的所有的评论信息列表，可翻页。评论信息包括书名、用户名、用户评分、评论日期、评论内容、评论的点赞数（有用数）等信息。

爬取标签链接、图书列表、图书详情、图书评论页面的爬虫代码分别是testLink.py , testBookList.py, testEntry.py, testComment.py。

在技术上实现用Scrapy框架爬取数据时，原理就是首先用选择器来定位要爬取的元素的位置，然后说明要爬取的元素的属性，即可爬取到想要的数据。

选择器有两种方案，一种是xpath选择器，一种是CSS选择器。最初，我选择了xpath选择器，xpath是基于位置的选择器，优点是容易获取路径，可以直接在浏览器内获取。缺点是xpath是基于路径的，而豆瓣读书中不同的网页可能显示不同的内容。如，一些热销书的图书详情页面非常全面，包括十种信息。但是一些冷门的书籍，它的信息可能就非常少了，只有三种。再比如显示一个标签的图书的列表，正常的话一个页面能够显示19本书，但是如果翻到了最后一页，可能只显示不到19本书。如果网页内缺失要爬的数据，则导致的结果是我的xpath并没有选择到我需要的元素，而是定位到了这个元素后面的元素，或者是定位不到xpath指定的元素，抛出list的下标溢出异常。

在爬虫过程中，翻页功能采用的scrapy框架中的Response.urljoin()和Request(next_page, callback=self.parse_list)函数。比如我在解析图书列表时，需要进行翻页，使用CSS选择器找到下一页的链接next_page后，使用Response.urljoin()将当前页面与next_page进行对比和按照网页链接的规律组合。返回的结果是下一页的链接link，接着再用Request(next_page, callback=self.parse_list)，对next_page链接使用parse_list解析函数，也就是解析这个图书列表页面的爬虫函数。总结一下，也就是说，整个爬虫的逻辑是一个递归函数：在一个函数内，爬取当前页面的数据并进行数据的存储，然后对这个页面的下一页，再用同样的解析函数进行处理，依次类推，对所有的页面都进行同样的处理。这样就实现了相同标签的所有图书信息的爬取。
	
在我写的爬虫中，所有的翻页都是一样的原理。爬取每本图书的评论列表的翻页功能也是这样实现的。用一个解析函数，先处理第一页的内容，然后进行数据的持久化，再用同样的函数处理下一页，以此类推，对所有的评论页面都用同样的解析函数进行处理。如果处理到最后一页，则会用if else语句进行判断是否有下一页，如果没有下一页不会再调用处理下一页的函数。

豆瓣读书中，也有些页面是不存在的，可能是豆瓣官方的服务器问题。如：在标签为“小说”的图书列表中，https://book.douban.com/tag/%E5%B0%8F%E8%AF%B4
，该页面会显示这个标签下共有100页的图书列表，但是经过调试，修改网页链接中的起始书籍数，发现小说列表下仅有50页的图书，从第51页开始就会显示“没有找到符合条件的图书”。这个也是需要用代码进行判断，如果显示“没有找到符合条件的图书”，则不再进行下一页的爬取，即终止对当前标签的图书列表的爬取。开始爬取下一个图书列表。

## 3.2 爬虫代码清单

### 3.2.1 testLink.py

    #-*- coding: UTF-8 -*-

    import scrapy
    import sys
    from tutorial.items import Book
    from tutorial.items import Comment

    class doubanSpider(scrapy.Spider):

        # link --> booklist --> entry(写入) --> comment(写入)
        # parse    parse_list   parse_entry    parse_comment

        name = 'tagLink'
        start_urls = ['https://book.douban.com/tag/?view=type&icn=index-sorttags-hot']

        def parse(self, response):
            lista = response.css('table.tagCol a::attr(href)')
            
            with open('data/my_taglink.txt', 'a') as f:
                for href in response.css('table.tagCol a::attr(href)').extract():
                    href = response.urljoin(href)
                    f.write(href + '\n')
            
            for href in response.css('table.tagCol a::attr(href)').extract():
                href = response.urljoin(href)
                # link --> booklist
                yield scrapy.Request(href, callback=self.parse_list)

### 3.2.2 testBookList.py

    #-*- coding: UTF-8 -*-
    import scrapy
    import sys
    from tutorial.items import Book

    class doubanSpider(scrapy.Spider):
        
        # link --> booklist --> entry(写入) --> comment(写入)
        name = 'testBookList'

        start_urls = [
            'https://book.douban.com/tag/%E5%B0%8F%E8%AF%B4'
        ]

        def parse_list(self, response):

            ###################### 如果下一页为空，则爬取下一个tag #########################
            ###########################################################################
            
            ifbook = response.css('p.pl2::text')
            if ifbook.extract()[0]=='没有找到符合条件的图书':
                print('no more booklist!')
            
            ###########################################################################
            ###########################################################################
            
            else:
                booklist = response.css('li.subject-item')

                for book in booklist:
                    booklink = book.css('h2 a::attr(href)').extract()[0]
                    bookname = book.css('h2 a::attr(title)').extract()[0]
                    bookinfo = book.css('div.pub::text').extract()[0].replace('\n','').strip()
                    img = book.css('img::attr(src)').extract()[0]
                    bookrate = book.css('span.rating_nums::text').extract()[0]
                    bookstar = book.css('div.star.clearfix span::attr(class)')[0].extract()
                    with open('data/my_bookinfo.txt', 'a') as f:
                        f.write(bookname + '    ' \
                                + booklink + '    ' \
                                + bookinfo + '    ' \
                                + img + '    ' \
                                + bookrate + '    ' \
                                + bookstar + '    ' \
                                + '\n')
                    # booklist --> entry
                    yield scrapy.Request(next_page, callback=self.parse_entry)
            
                next_page = response.css('span.next a::attr(href)').extract()[0]
                if next_page is not None:
                    next_page = response.urljoin(next_page)
                    yield scrapy.Request(next_page, callback=self.parse_list)

### 3.2.3 testEntry.py

    #-*- coding: UTF-8 -*-
    import scrapy
    import sys
    from tutorial.items import Book


    class doubanSpider(scrapy.Spider):
        
        name = 'testEntry'

        start_urls = [
            'https://book.douban.com/subject/25862578/'
        ]

        def parse_entry(self, response):

            # book = Book()

            ############################## info from entry ############################
            ###########################################################################
            
            # title = response.css('div#info span.pl::text').extract()
            # for i in title:
            #     i.replace(':','').replace('：','')
            # content = response.css('div#info::text').extract()
            # for i in content:
            #     i.strip().replace('\n', '').replace
            
            ###########################################################################
            ###########################################################################
            

            # book['name'] = response.xpath('//*[@id="wrapper"]/h1/span')
            # book['author'] = response.xpath('//*[@id="info"]/a[1]/text()').extract()[0].strip().replace('\n','').replace(' ','')
            # book['public'] = response.xpath('//*[@id="info"]/text()[5]').extract()[0].replace(' ','')
            # book['origin_name'] = response.xpath('//*[@id="info"]/text()[7]').extract()[0].replace(' ','')
            # book['public_year'] = response.xpath('//*[@id="info"]/text()[10]').extract()[0].replace(' ','')
            # book['pages'] = response.xpath('//*[@id="info"]/text()[12]').extract()[0].replace(' ','')
            # book['price'] = response.xpath('//*[@id="info"]/text()[14]').extract()[0].replace(' ','')
            # book['book_type'] = response.xpath('//*[@id="info"]/text()[16]').extract()[0].replace(' ','')
            # book['isbn'] =  response.xpath('//*[@id="info"]/text()[20]').extract()[0].replace(' ','')
            # book['comment_link'] = response.xpath('//*[@id="content"]/div/div[1]/div[3]/div[11]/h2/span[2]/a/@href').extract[0]

            # yield book

            link = response.css('div.mod-hd h2 span.pl a::attr(href)').extract()[0]
            comment_link = response.urljoin(link)
            yield scrapy.Request(comment_link, callback=self.parse_comment)


### 3.2.4 items.py （持久化相关代码）

    # -*- coding: utf-8 -*-

    # Define here the models for your scraped items
    #
    # See documentation in:
    # https://doc.scrapy.org/en/latest/topics/items.html

    import scrapy


    class TutorialItem(scrapy.Item):
        # define the fields for your item here like:
        # name = scrapy.Field()
        pass

    class Comment(scrapy.Item):
        book = scrapy.Field()
        user = scrapy.Field()
        rate = scrapy.Field()
        date = scrapy.Field()


    class Book(scrapy.Item):
        tag = scrapy.Field()
        name = scrapy.Field()
        rate = scrapy.Field()
        comment_num = scrapy.Field()
        rate5 = scrapy.Field()
        rate4 = scrapy.Field()
        rate3 = scrapy.Field()
        rate2 = scrapy.Field()
        rate1 = scrapy.Field()
        author = scrapy.Field()
        original_name = scrapy.Field()
        translator = scrapy.Field()
        public_year = scrapy.Field()
        pages = scrapy.Field()
        price = scrapy.Field()
        bookType = scrapy.Field()
        packing = scrapy.Field()
        ISBN = scrapy.Field()
        comment_link = scrapy.Field()


### 3.2.5 pipelines.py（持久化相关代码）

    # -*- coding: utf-8 -*-

    # Define your item pipelines here
    #
    # Don't forget to add your pipeline to the ITEM_PIPELINES setting
    # See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html


    class TutorialPipeline(object):
        def process_item(self, item, spider):
            return item


    class CommentPipeline(object):
        
        """生成txt文件!"""
        
        def open_spider(self, spider):
            self.f = open('data/comment.txt', 'a')
        
        def process_item(self, Comment, spider):
            content = str(Comment['book']) + '\t' + str(Comment['user']) + '\t' + str(Comment['rate']) + '\t' + str(Comment['date']) + '\n'
            self.f.write(content)  #python3
            return Comment
        
        def close_spider(self, spider):
            self.f.close()


    class BookPipeline(object):
        
        """生成txt文件!"""
        
        def open_spider(self, spider):
            self.f = open('data/book.txt', 'a')
        
        def process_item(self, Book, spider):
            # content = str(Book['name']) + '\t' \
            #          + str(Book['tag']) + '\t' \
            #          + str(Book['author'])  + '\t' \
            #          + str(Book['public']) + '\t' \
            #          + str(Book['origin_name']) + '\t' \
            #          + str(Book['public_year']) + '\t' \
            #          + str(Book['pages']) + '\t' \
            #          + str(Book['price']) + '\t' \
            #          + str(Book['book_type']) + '\t' \
            #          + str(Book['isbn']) + '\n'
            # self.f.write(content)
            return Book
        
        def close_spider(self, spider):
            self.f.close()

### 3.2.6 settings.py（设置反爬机制）

    # -*- coding: utf-8 -*-

    # Scrapy settings for tutorial project
    #
    # For simplicity, this file contains only settings considered important or
    # commonly used. You can find more settings consulting the documentation:
    #
    #     https://doc.scrapy.org/en/latest/topics/settings.html
    #     https://doc.scrapy.org/en/latest/topics/downloader-middleware.html
    #     https://doc.scrapy.org/en/latest/topics/spider-middleware.html

    BOT_NAME = 'tutorial'

    SPIDER_MODULES = ['tutorial.spiders']
    NEWSPIDER_MODULE = 'tutorial.spiders'


    # Crawl responsibly by identifying yourself (and your website) on the user-agent
    USER_AGENT = 'tutorial (+http://www.yourdomain.com)'

    USER_AGENTS = [
        "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)",
        "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)",
        "Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)",
        "Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)",
        "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)",
        "Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)",
        "Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)",
        "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)",
        "Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6",
        "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1",
        "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0",
        "Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5",
        "Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6",
        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20",
        "Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52",
    ]

    PROXIES = [
        {'ip_port': '111.11.228.75:80', 'user_pass': ''},
        {'ip_port': '120.198.243.22:80', 'user_pass': ''},
        {'ip_port': '111.8.60.9:8123', 'user_pass': ''},
        {'ip_port': '101.71.27.120:80', 'user_pass': ''},
        {'ip_port': '122.96.59.104:80', 'user_pass': ''},
        {'ip_port': '122.224.249.122:8088', 'user_pass': ''},
    ]

    # Obey robots.txt rules
    ROBOTSTXT_OBEY = False

    # Configure maximum concurrent requests performed by Scrapy (default: 16)
    #CONCURRENT_REQUESTS = 32

    #USER_AGENT = 'tutorial (+http://www.yourdomain.com)'
    # Configure a delay for requests for the same website (default: 0)
    # See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay
    # See also autothrottle settings and docs
    DOWNLOAD_DELAY = 3
    # The download delay setting will honor only one of:
    #CONCURRENT_REQUESTS_PER_DOMAIN = 16
    #CONCURRENT_REQUESTS_PER_IP = 16

    # Disable cookies (enabled by default)
    COOKIES_ENABLED = False

    # Disable Telnet Console (enabled by default)
    #TELNETCONSOLE_ENABLED = False

    # Override the default request headers:
    #DEFAULT_REQUEST_HEADERS = {
    #   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    #   'Accept-Language': 'en',
    #}

    # Enable or disable spider middlewares
    # See https://doc.scrapy.org/en/latest/topics/spider-middleware.html
    #SPIDER_MIDDLEWARES = {
    #    'tutorial.middlewares.TutorialSpiderMiddleware': 543,
    #}

    # Enable or disable downloader middlewares
    # See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html
    #DOWNLOADER_MIDDLEWARES = {
    #    'tutorial.middlewares.TutorialDownloaderMiddleware': 543,
    #}

    DOWNLOADER_MIDDLEWARES = {
    #    'cnblogs.middlewares.MyCustomDownloaderMiddleware': 543,
        #'cnblogs.middlewares.RandomUserAgent': 1,
        #'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 110,
        'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 110,
        # 'street.middlewares.RandomUserAgentMiddleware': 400,
        # 'street.middlewares.RedirectMiddleware': 100,
        'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': None,
        #'cnblogs.middlewares.ProxyMiddleware': 100,
    }

    # Enable or disable extensions
    # See https://doc.scrapy.org/en/latest/topics/extensions.html
    #EXTENSIONS = {
    #    'scrapy.extensions.telnet.TelnetConsole': None,
    #}

    # Configure item pipelines
    # See https://doc.scrapy.org/en/latest/topics/item-pipeline.html
    ITEM_PIPELINES = {
    'tutorial.pipelines.TutorialPipeline': 300,
    'tutorial.pipelines.CommentPipeline': 200,
    'tutorial.pipelines.BookPipeline': 200,
    }

    # Enable and configure the AutoThrottle extension (disabled by default)
    # See https://doc.scrapy.org/en/latest/topics/autothrottle.html
    #AUTOTHROTTLE_ENABLED = True
    # The initial download delay
    #AUTOTHROTTLE_START_DELAY = 5
    # The maximum download delay to be set in case of high latencies
    #AUTOTHROTTLE_MAX_DELAY = 60
    # The average number of requests Scrapy should be sending in parallel to
    # each remote server
    #AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0
    # Enable showing throttling stats for every response received:
    #AUTOTHROTTLE_DEBUG = False

    # Enable and configure HTTP caching (disabled by default)
    # See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings
    #HTTPCACHE_ENABLED = True
    #HTTPCACHE_EXPIRATION_SECS = 0
    #HTTPCACHE_DIR = 'httpcache'
    #HTTPCACHE_IGNORE_HTTP_CODES = []
    #HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'



# 4 数据批处理及推荐算法的实现

## 4.1 处理数据前的分析
得到的数据集的格式如下：

![](http://ww1.sinaimg.cn/large/005N2p5vgy1frdarjfjgcj31440oedx6.jpg)

为了方便计算，将第一列的userid转换成int类型：首先根据userid进行去重，得到唯一的userid，然后对每一个userid生成一个自增的int型主键，得到一个用户主键文件，含有列（userid，intuserid）。再将原始文件与新得到的主键文件进行表的左连接（使用Spark处理）



## 4.2 数据的清洗及转换

### 4.2.1 首先按照user进行行去重

        # coding: UTF-8
        #-*- coding: UTF-8 -*-
        from pyspark.sql import Row
        from pyspark import SparkContext, SparkConf
        from pyspark.sql import SparkSession


        if __name__ == "__main__":
            
            ###############################################
            ################ 对user进行去重 #################
            ###############################################
            
            # spark = SparkSession.builder.appName("useridint").getOrCreate()
            conf = SparkConf().setAppName("getpk").setMaster("local")
            sc = SparkContext(conf=conf)

            lines = sc.textFile('data/rating_Books.csv')
            lines = lines.map(lambda x:x.split(','))
            lines.count()
            # shell调试运行时，显示 22507155

            kv = lines.map(lambda x:(x[0],x))
            k = kv.reduceByKey(lambda x,y:x)
            key = k.map(lambda x:x[0])
            key.count()
            # shell调试运行时，显示 8026324

            key.take(5)
            key.saveAsTextFile('data/userkeys.txt')
![](http://ww1.sinaimg.cn/large/005N2p5vly1frfah9g4gtj30vk066wft.jpg)

![](http://ww1.sinaimg.cn/large/005N2p5vly1frfalfa7k6j30w60segpq.jpg)

![](http://ww1.sinaimg.cn/large/005N2p5vly1frfamq4g0wj32801coahf.jpg)

bookkeys样本数据

![](http://ww1.sinaimg.cn/large/005N2p5vly1frfaol0qutj32801coqd0.jpg)


### 4.2.2 对每个user生成一个int类型的主键


        from pyspark.sql import Row
        from pyspark import SparkContext, SparkConf
        from pyspark.sql import SparkSession


        if __name__ == "__main__":
            # spark = SparkSession.builder.appName("useridint").getOrCreate()
            conf = SparkConf().setAppName("useridint").setMaster("local")
            sc = SparkContext(conf=conf)

            lines = sc.textFile('data/userkeys.txt/part*')

            n = 1
            a = sc.accumulator(1)
            def incre(row):
                row = str(a) + ',' + row
                a.add(1)
                return row

            newlines = lines.map(lambda x: incre(x))
            newlines.saveAsTextFile('data/userintid')

 得到的（intid,userid）文件部分内容

    ![](http://ww1.sinaimg.cn/large/005N2p5vly1frdey3f2drj31gi0vyqdj.jpg)

 得到的（intid,bookid）文件部分内容

    ![](http://ww1.sinaimg.cn/large/005N2p5vly1frfbn0rp22j32801cogww.jpg)



### 4.2.3 Spark core进行批处理得到推荐数据

    from pyspark.ml.evaluation import RegressionEvaluator
    from pyspark.ml.recommendation import ALS
    from pyspark.sql import Row
    from pyspark import SparkContext, SparkConf
    from pyspark.sql import SparkSession

    def clearstr(x):
        try:
            x[1]=int(x[1])
            return x
        except:
            return ''

    spark = SparkSession.builder.appName("testcf").getOrCreate()
    lines = spark.read.text("data/recinfo/part*").rdd
    parts = lines.map(lambda row: row.value.split(","))
    ratingsRDD = parts.map(lambda p: Row(userId=int(p[4]), BookId=int(p[1]),
                                        rating=float(p[2]), timestamp=long(p[3])))
    ratings = spark.createDataFrame(ratingsRDD)
    (training, test) = ratings.randomSplit([0.8, 0.2])

    # Build the recommendation model using ALS on the training data
    # Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics
    # als = ALS(maxIter=5, regParam=0.01, userCol="userId", itemCol="BookId", ratingCol="rating",
    #           coldStartStrategy="drop")
    als = ALS(maxIter=5, regParam=0.01, implicitPrefs=True,
            userCol="userId", itemCol="BookId", ratingCol="rating")
    model = als.fit(training)

    # Evaluate the model by computing the RMSE on the test data
    predictions = model.transform(test)
    evaluator = RegressionEvaluator(metricName="rmse", labelCol="rating",
                                    predictionCol="prediction")
    rmse = evaluator.evaluate(predictions)
    print("Root-mean-square error = " + str(rmse))

    # Generate top 10 book recommendations for each user
    userRecs = model.recommendForAllUsers(10)
    # userRecs.saveAsTable("testcfoutput")
    userRecs.rdd.saveAsTextFile('data/testbookrec')
    # Generate top 10 user recommendations for each book
    bookRecs = model.recommendForAllItems(10)

    # Generate top 10 book recommendations for a specified set of users
    users = ratings.select(als.getUserCol()).distinct().limit(3)
    userSubsetRecs = model.recommendForUserSubset(users, 10)
    # Generate top 10 user recommendations for a specified set of books
    books = ratings.select(als.getItemCol()).distinct().limit(3)
    bookSubSetRecs = model.recommendForItemSubset(books, 10)



# 5 web端的搭建及与批处理数据的连接


## 5.1 概述

前端及响应网站的服务器搭建在bishe主机中（centos7.3）。MySQL数据库也建立在bishe主机上。spark主机用来存放批处理之后的数据（为每一个用户推荐10本可能喜欢的图书）
批处理端将预先处理好的数据放到bishe主机的MySQL数据库中，web端再从MySQL数据库中存取数据。


## 5.2 web端设计

首先使用python将爬虫、批处理得到的txt文本保存到MySQL数据库中。再使用flask框架，将数据传输到前端。

其次是web端用来展示信息，web端部署在bishe服务器上，用来加速存取的MySQL数据库也部署在bishe服务器上。服务器的表包括user表和book表。user表字段有用户名、用户密码等信息。book表信息有书名、链接、图片url、作者、出版社、出版年月等信息。web端展示的信息包括当前登录的用户信息、图书列表、为当前用户推荐的图书。

## 5.3 MySQL端存储的数据

MySQL存储的数据来自爬虫和亚马逊评论数据批处理之后的信息。

爬虫数据来自豆瓣读书。主要内容包括书籍的详细信息。如书名、链接、图片URL、作者、出版社、出版年月等信息。

批处理用到的信息来自UCSD的亚马逊公开数据集：
- 书籍评论信息：书id，用户id，评分，时间戳
- 书籍信息：书id，书籍其他具体信息（书名、类别等）

以及得到的推荐数据：用户id，书id


