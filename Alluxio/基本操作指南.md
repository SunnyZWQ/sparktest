# 在Alluxio上运行Spark

## 使用Alluxio中已有的文件

1. 将文件从本地拷贝到Alluxio中（在Alluxio_HOME）

        bin/alluxio fs copyFromLocal LICENSE /LICENSE

2. 在spark-shell中运行如下命令（假定Alluxio Master运行在localhost）

        > val s = sc.textFile("alluxio://localhost:19998/LICENSE")
        > val double = s.map(line => line + line)
        > double.saveAsTextFile("alluxio://localhost:19998/LICENSE2")

3. 运行结果可以在浏览器中查看，输入网址：http://localhost:19999/browse
可以看到输出的LICENSE2文件

## 使用HDFS上的数据

1. Alluxio挂载HDFS的某个目录，如：

        # 在Alluxio中建立目录/mnt用来挂载
        ./bin/alluxio fs mkdir /mnt
        
        # 将HDFS目录挂载到Alluxio目录中
        ./bin/alluxio fs mount alluxio://localhost:19998/mnt/hdfs hdfs://localhost:9000/alluxio/

2. 将文件上传到Alluxio挂载的HDFS的目录

        hdfs dfs -put -f ${ALLUXIO_HOME}/LICENSE hdfs://localhost:9000/alluxio/LICENSE_HDFS

    注意：Alluxio并没有文件的概念，如果想查看是否运行成功，需要去web UI中检查。

3. 使用spark处理Alluxio中的文件，并将结果也保存在Alluxio中

        > val s = sc.textFile("alluxio://localhost:19998/LICENSE_HDFS")
        > val double = s.map(line => line + line)
        > double.saveAsTextFile("alluxio://localhost:19998/LICENSE_HDFS2")

>NOTE: Block caching on partial reads is enabled by default, but if you have turned off the option, it is possible that the LICENSE_HDFS file is not in Alluxio storage. This is because Alluxio only stores fully read blocks, and if the file is too small, the Spark job will have each executor read a partial block. To avoid this behavior, you can specify the partition count in Spark. For this example, we would set it to 1 as there is only 1 block.


        > val s = sc.textFile("alluxio://localhost:19998/LICENSE_HDFS", 1)
        > val double = s.map(line => line + line)
        > double.saveAsTextFile("alluxio://localhost:19998/LICENSE_HDFS2")



# 常用连接
Alluxio shell API：
https://www.alluxio.org/docs/master/en/Command-Line-Interface.html

参考：https://www.alluxio.org/docs/1.7/en/Running-Spark-on-Alluxio.html
















