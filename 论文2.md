
# 目录

#### 1 课题背景（引出需求）

1.1 推荐系统简介

1.2 软件概述

#### 2 需求

2.1 数据

2.2 存储

2.3 web端

2.4 技术需求

2.4.1 数据源

2.4.2 爬取数据技术

2.4.3 大数据处理及推荐算法

2.4.4 web


#### 3 设计



#### 4 实现


#### 5 结果展现


---


# 1 课题背景

## 1.1 推荐系统简介

推荐系统是一种能够根据用户的过去行为来预测用户可能还喜欢什么产品。推荐系统普遍与电商系统相结合，通过客户端收集用户行为，并在后台使用推荐算法预测用户可能喜欢的产品，再将这些产品在客户端为用户进行展示，进而提高产品卖出的概率，增大平台销量。

推荐系统用于预测的算法是Amazon.com在1998年推出的基于项目的协同过滤算法（item-based collaborative filtering）。该算法使推荐系统能够基于上百万的商品目录为数百万用户提供推荐服务。其优点是简单、可扩展、能提供让人惊奇且有用的推荐、能根据用户的新信息快速地更新推荐结果以及能够以易于理解的方式解释推荐内容。

市面上常见的带有电商电商有亚马逊、淘宝、京东、当当等。各大电商网站都有自己的推荐系统看，来增加平台的销量。

## 1.2 软件概述

此课题将做出一个图书展示网站，为用户展示图书信息，并能为用户做出推荐。

图书展示网站将在前端为用户展示书籍列表。在后台，系统对来自豆瓣和UCSD的书籍评论信息作出处理，为每个用户推荐10本最可能喜欢的书籍。如果历史信息不足以推荐出10本，将尽可能推荐更多书籍。其中，书籍列表信息来自于从“豆瓣读书”爬到的数据。为用户做的推荐，一部分来自于“豆瓣读书”中爬到的评论信息，为了使推荐更准确，还使用了来自UCSD的公开数据集。

数据处理端的工作主要分为两大类，一是对采集到的数据进行清洗、整合，二是根据清洗好的数据为用户做推荐，并将推荐数据持久化，实现基于用户的协同过滤。

# 2 需求

## 2.1 数据来源

第一，对于展示图书信息的网站来说，需要图书的详细信息，如书名、作者、出版社、图片、价格等。第二，用于推荐的数据。推荐需要以往的用户行为，但不会有网站会公布这样的信息。因此，最合适的就是网站中展示的用户评论数据。评论数据，需要包含三个内容，分别是用户id，书id，评分。用户评论数据非常重要的一点是用户id或者用户名是唯一的，如果网站不展示用户名的全名，那么可能造成不同的用户会展示出相同的用户名。

最终，经过对各大电商网站的调研，发现“豆瓣读书”网站展示了较全面的书籍信息，并且书籍的评论数据展示了用户的全名及评分。因此，决定爬取豆瓣读书中的数据。

考虑到推荐数据的准确性，还需要更多的评论数据。协同过滤是机器学习算法中的一种，对于机器学习来说，历史数据越多，建立的模型就更完善，推荐出的数据也就越准确。因此，为了有更准确的推荐结果，还选取了来自UCSD的亚马逊公开数据集中的书籍评论信息。


## 2.2 数据处理及持久化

**1.用于推荐的数据**

进行数据处理的推荐算法选用基于用户的协同过滤。基于用户的协同过滤的算法的原理是通过计算用户与用户之间的相似度，来为用户推荐可能喜欢的书籍。这个算法的大致流程是：首先根据过往的用户行为，比如点击、搜索、收藏、付款，为每一个行为设计权重，然后将用户对每一件商品的喜好度评分根据各个行为的权重计算出来，形成一个（用户-物品）的评分矩阵。再用评分矩阵，计算用户与用户的相似度，利用的数学原理是向量与向量之间的余弦夹角。

在做推荐前首先要对数据进行清洗。推荐算法最基础的需要是用户id、书籍id和评分。在对算法进行优化时，还需要考虑到算法的效率和容错性。对于算法的效率，需要用户id和书籍id都是int型书籍，这样可以在数据处理中进行排序，以加快处理效率。对于程序的容错性，需要使每一行信息都是唯一的，所以又在算法中加入了int型的时间戳。综上，用于推荐的数据需要具有如下的列：用户id（int型）、书id（int型）、评分（float型）、时间戳（int型）。

分析获取到的数据，现在的列分别为用户id（String型）、书id（String型）、评分（float型）、时间戳（int型），因此根据推荐算法的数据需求，还需要为string类型的用户id和书id分别生成对应的int型id。


**2.展示在web端的图书信息**

用于展示在web端的图书信息都来自于从豆瓣读书获取到的信息，因此可以按照需求进行数据采集及持久化，不需要额外的数据处理。

## 2.3 web端

web端的主要功能是能展示图书信息，并能在用户登录后进入的首页上，为用户展示根据其过往的行为得到的为其推荐的三本书籍。

因此，web端需要展示从“豆瓣读书”中获取到的数据。主要内容包括书籍的详细信息。如书名、链接、图片URL、作者、出版社、出版年月等信息。此数据由爬虫代码持久化到数据库中及文本文件中。推荐数据需要在批处理端处理过之后进行持久化，也是持久化到数据库中及文本文档中。

持久化到数据库是为了加速web端响应速度，持久化到文本文档中是为了工程的可扩展性，文本文档可以方便下一次的批处理。


## 2.4 技术需求

### 2.4.1 爬取数据技术

1.技术概述

- 操作系统：CentOS 7.3
- 语言：python
- 框架：Scrapy

2.Scrapy爬虫

Scrapy框架的原理是简化了从网站获取元素内容的代码量。将本来复杂繁琐的Beaufiful Soup代码简化到只要用固定的格式就能抽取要获取的网站的元素，及其内容。

用来定位元素位置的选择器有两种方案，一种是xpath选择器，一种是CSS选择器。

两种选择器各有优势，xpath的优势是方便获取，缺点是对于网站布局在动态改变的情况，xpath的适应情况会差一点。CSS选择器的缺点是定位相对xpath复杂一些，但是也很容易获取，所以也不算是很困难。优点就是可以按照内容来爬取数据，而不是按照位置，这样大大增加了爬虫的灵活性，更能适应动态改变布局的网站。

综合考虑，选择用CSS选择器来爬取豆瓣读书的信息。

### 2.4.2 大数据处理

在处理数据及实现算法时，用到的是当前流行的Spark。Spark是独立于Hadoop的一个计算框架，但是它可以与Hadoop共同工作，如使用Hadoop中的存储系统HDFS。Spark的主要作用是计算。在大数据中，用于计算的组件一个是MapReduce，另一个是Spark。

Hadoop解决了大数据的处理问题主要是通过两方面——存储和计算。
Hadoop解决大数据计算问题的组件是MapReduce。MapReduce是最早期用于批处理的组件。它为开发者提供了一个编程模型。开发者只需要使用这个编程模型实现数据的计算，而不需要考虑分布式如何实现、如何容错等问题。

MapReduce解决了大数据的计算问题，但它并不是完美的，MapReduce的计算速度太慢了，如果对数据的处理时间有要求。比如说实时计算，那么MapReduce就不能满足需求。

MapReduce之所以计算速度特别慢，是因为它每次在计算的时候，首先从硬盘中取数据到内存，计算出中间结果之后，再将中间结果重新写入硬盘中。如果需要对中间结果进行下一步计算，又需要将中间结果重新从硬盘中读出并写入内存。
也就是说，MapReduce在执行过程中浪费了大量的I/O时间。

为了解决MapReduce进行大数据批处理时运算速度过慢的问题，Spark对其进行了很多改进。

Spark在进行批处理时主要用到的是Spark core。首先，是运算速度方面。刚才已经提到MapReduce在执行过程中浪费了大量的I/O时间，因此，Spark的改进就是中间结果不再放到硬盘中，而是继续存放在内存中。这样，下次再需要中间结果的时候就不需要消耗I/O时间。

Spark做的第二个改进就是它提供的并不是单一的编程模型，而是可以使用自定义函数。这样就大大增加了Spark的易用性。MapReduce提供的编程模型的逻辑非常简单，因此要实现非常复杂的逻辑，就要把这个简单的功能一层一层叠加。既然要叠加逻辑，就要考虑每一层之间的连接，如何设计每一层逻辑，来使这个编程模型更高效。因此，MapReduce 的易用性非常差——仅仅使用MapReduce进行表的关联就需要非常复杂的逻辑。
	
接下来是Spark的第三个改进，就是Spark框架为批处理、交互式、流式、机器学习、图计算提供一个统一的数据处理平台，这相对于使用Hadoop有很大优势。Hadoop在使用时，如果想使用其余的组件，需要单独对这个组件配置环境，并且要考虑各个组件之间如何连接，如何高效的连接。	

综合以上三点，可以看出无论是从运算速度还是易用性等各种方面，Spark都显著优于MapReduce。因此，批处理及推荐算法的实现都选用了Spark作为计算组件。

### 2.4.3 推荐算法

#### 人工智能及机器学习简介

协同过滤算法是机器学习众多算法之一。最基础的机器学习的流程如下图。

![](http://ww1.sinaimg.cn/large/005N2p5vgy1fpy4h98u3kj30in0a4taf.jpg)

人工智能 指的就是 像人一样的思考、行为。而且是理性的思考，理性的行为。人在生活工作中会不断学习，但是机器不会。机器不能进行自身的调整以适应新的变化。因此，机器学习解决的问题就是——让计算机系统和人工智能系统具备学习环境，根据现行的工作环境进行调整以适应生产和科技的。
	
机器学习是实现人工智能的一个途径，即以机器学习为手段解决人工智能中的问题。学习就是系统经过不断重复的工作逐步改进和增强自身能力的过程，当系统在下一次遇到相同或相似的问题时，会比现在处理的更好或者更高效。
	
机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。因为学习算法中涉及了大量的统计学理论，机器学习与推断统计学联系尤为密切，也被称为统计学习理论。

机器学习的过程是：

1. 先对一些训练数据集使用机器学习的算法进行处理，找出数据的规律，找到的规律就是模型。
2. 再用模型对新的数据进行处理，就能得到预测数据。

#### 基于用户的协同过滤的算法的原理

基于用户的协同过滤的算法的原理是通过计算用户与用户之间的相似度，来为用户推荐可能喜欢的书籍。这个算法的大致流程是：首先根据过往的用户行为，比如点击、搜索、收藏、付款，为每一个行为设计权重，然后将用户对每一件商品的喜好度评分根据各个行为的权重计算出来，形成一个（用户-物品）的评分矩阵。再用评分矩阵，计算用户与用户的相似度，利用的原理是向量与向量之间的余弦夹角。
![](http://ww1.sinaimg.cn/large/005N2p5vgy1fremljhvaoj311s0gqwie.jpg)


### 2.4.4 web框架

1.技术综述

- 操作系统：CentOS 7.3
- 语言：python
- 框架：flask
- 前端：HTML，CSS
- 数据库：MySQL
- Kappa架构的思想

2.flask框架及前端技术

前端采用HTML、CSS写网页，后端使用Python编写。Python编写的后台使用到的框架是flask。Python有很多搭建网站的框架，流行的如Django等，flask更轻量一些，Django虽然功能全但是更复杂，仅仅搭建一个能连接数据库的后台不需要过多的功能。因此，服务器端选择使用flask框架。

3.Kappa架构的思想

采用Kappa架构的思想主要是为了加速web端的响应速度。如果服务器端收到推荐数据请求，批处理端服务器就进行批处理运算，那么会极大加长服务器响应时间，甚至是超时而无法响应用户请求，会造成极差的用户体验。因此，采用Kappa架构的思想，将批处理后的数据存储到MySQL数据库中，这样可以加速服务器获取数据的速度。

# 3 设计

## 3.1 “豆瓣读书”的数据获取

### 3.1.1 爬虫程序的设计综述

获取豆瓣读书网站的图书信息的步骤如下：

首先进入标签链接页面，此页面包含豆瓣中所有的图书标签和对应的链接，从标签点击进入链接网页，即可进入图书列表页面。图书列表页面展示所有图书的概要信息，点击每一项图书，即可进入该图书的详细信息页面。在详细信息页面，可找到评论页面的链接。再进入评论页面，即可展示此本书的所有评论。

### 3.1.2 代码功能
1、标签链接页面(testLink.py)。爬数据的初始网址中包含豆瓣中所有的图书标签和对应的链接，从标签点击链接就能看到这个标签下所有的图书的可翻页的列表。在这个页面中爬取所有图书标签的链接地址。

2、图书列表页面（可翻页）(testBookList.py)。 在此列表中，可以抓取基本信息，例如指向书籍详细信息页面的链接，标题，作者，翻译员，出版商，出版日期和图书价格。

3、书籍详情页面(testEntry.py)。在此页面获取到评论页面的链接。

4、数据评论页面(testComment.py)。在评论页面能够看到本书的所有的评论信息列表，可翻页。评论信息包括书名、用户名、用户评分、评论日期、评论内容、评论的点赞数（有用数）等信息。

5、设置反爬机制(settings.ps)。在Scrapy框架中，设置反爬机制的代码主要在settings.py文件中。设置反爬机制的目的是将程序伪装成真正的用户。主要原理就是控制爬取页面的延时、多设置用户代理、网络代理。否则会被豆瓣检测出可能是爬虫程序。

爬取标签链接、图书列表、图书详情、图书评论页面的爬虫代码分别是testLink.py , testBookList.py, testEntry.py, testComment.py。






## 3.3 web端的设计

1.概述

前端及响应网站的服务器搭建在bishe主机中（centos7.3）。MySQL数据库也建立在bishe主机上。spark主机用来存放批处理之后的数据（为每一个用户推荐10本可能喜欢的图书） 批处理端将预先处理好的数据放到bishe主机的MySQL数据库中，web端再从MySQL数据库中存取数据。

2.web端架构

首先使用python将爬虫、批处理得到的txt文本保存到MySQL数据库中。再使用flask框架，将数据传输到前端。
其次是web端用于显示信息，web端部署在bishe服务器上，用来加速存取的MySQL数据库也部署在bishe服务器上。服务器的表包括user表和book表。user表字段有用户名、用户密码等信息。book表信息有书名、链接、图片url、作者、出版社、出版年月等信息。web端展示的信息包括当前登录的用户信息、图书列表、为当前用户推荐的图书。

3.MySQL端存储的数据

MySQL存储的数据来自爬虫和亚马逊评论数据批处理之后的信息。
爬虫数据来自豆瓣读书。主要内容包括书籍的详细信息。如书名、链接、图片URL、作者、出版社、出版年月等信息。
批处理用到的信息来自豆瓣读书和UCSD的亚马逊公开数据集：
书籍评论信息：书id，用户id，评分，时间戳
书籍信息：书id，书籍其他具体信息（书名、类别等）
以及得到的推荐数据：用户id，书id

# 4 实现

## 4.1 爬虫

### 4.1.1 选择器

在技术上实现用Scrapy框架爬取数据时，原理就是首先用选择器来定位要获取的元素的位置，然后解释要获取的元素的属性，即可获取到想要的数据。

选择器有两种方案，一种是xpath选择器，一种是CSS选择器。最初选择了xpath选择器，xpath是基于位置的选择器，优点是容易获取路径，可以直接在浏览器内获取。缺点是xpath是基于路径的，而豆瓣读书中不同的网页可能显示不同的内容。如，一些热销书的图书详情页面非常全面，包括十种信息。但是一些冷门的书籍，它的信息可能就非常少了，只有三种。再比如显示一个标签的图书的列表，正常的话一个页面能够显示19本书，但是如果翻到了最后一页，可能只显示不到19本书。如果网页内缺失要爬的数据，则导致的结果是我的xpath并没有选择到我需要的元素，而是定位到了这个元素后面的元素，或者是定位不到xpath指定的元素，抛出list的下标溢出异常。


### 4.1.2 翻页功能的实现

在信息获取过程中，翻页功能采用的scrapy框架中的Response.urljoin()和Request(next_page, callback=self.parse_list)函数。比如我在解析图书列表时，需要进行翻页，使用CSS选择器找到下一页的链接next_page后，使用Response.urljoin()将当前页面与next_page进行对比和按照网页链接的规律组合。返回的结果是下一页的链接link，接着再用Request(next_page, callback=self.parse_list)，对next_page链接使用parse_list解析函数，也就是解析这个图书列表页面的爬虫函数。

综上，整个爬虫的逻辑是一个递归函数：在一个函数内，爬取当前页面的数据并进行数据的存储，然后对这个页面的下一页，再用同样的解析函数进行处理，依次类推，对所有的页面都进行同样的处理。这样就实现了相同标签的所有图书信息的爬取。

在获取豆瓣读书的信息的过程中，所有的翻页都是一样的原理。获取每本图书的评论列表的翻页功能也是这样实现的。用一个解析函数，先处理第一页的内容，然后进行数据的持久化，再用同样的函数处理下一页，以此类推，对所有的评论页面都用同样的解析函数进行处理。如果处理到最后一页，则使用if else语句进行判断是否有下一页，没有下一页就不会再调用函数。

### 4.1.3 获取标签链接

### 4.1.4 获取标签页内所有图书列表信息

以下代码是获取一个标签页内的所有图书列表信息，在这里可以爬取详细的图书信息，和数据详细页面信息。
在这个列表中，也有些页面是不存在的，可能是豆瓣官方的服务器问题。如：在标签为“小说”的图书列表中，如https://book.douban.com/tag/%E5%B0%8F%E8%AF%B4 ，该页面会显示这个标签下共有100页的图书列表，但是经过调试，修改网页链接中的起始书籍数，发现小说列表下仅有50页的图书，从第51页开始就会显示“没有找到符合条件的图书”。这个也是需要用代码进行判断，如果显示“没有找到符合条件的图书”，则不再进行下一页的爬取，即终止对当前标签的图书列表的爬取。开始爬取下一个图书列表。

### 4.1.5 获取图书详情页面

这段代码用来获取图书详情页面，主要任务是获取图书的评论页面的链接，这样就能通过评论链接来获取此图书的所有评论信息。

### 4.1.6 获取图书评论信息

### 4.1.7 持久化的模型

items.py是Scrapy框架提供的建立Model的文件。这里会写书籍、评论等的多个成员变量，用来在pipelines.py中进行持久化。在这里写Model的原因是使代码更容易维护，增大可扩展性。

### 4.1.8 持久化的相关代码

这里的代码是将爬取到的数据持久化到文件或者数据库。因为我在批处理时主要使用txt文件，所以这里存储为文本文件。

### 4.1.9 设置反爬机制的代码


## 4.2 数据批处理及推荐算法的实现

### 4.2.1 数据处理综述

数据的格式为：第一列是书籍id，第二列为用户id，第三列为用户评分，第四列为时间戳。为了方便计算，将第一列的userid转换成int类型：首先根据userid进行去重，得到唯一的userid，然后对每一个userid生成一个自增的int型主键，得到一个用户主键文件，含有列（userid，intuserid）。再将原始文件与新得到的主键文件进行表的左连接（使用Spark处理）。


### 4.2.2 首先按照user进行去重

### 4.2.3 对每个user生成一个int类型的主键

### 4.2.4 Spark Core进行批处理得到推荐数据


# 5 结果展现

## 5.1 部分“豆瓣读书”数据展示

## 5.2 部分批处理结果展示

## 5.3 推荐算法的准确性验证

## 5.4 web端页面展示









































